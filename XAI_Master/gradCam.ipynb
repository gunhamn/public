{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import time\n",
    "import pytorch_grad_cam\n",
    "import cv2\n",
    "\n",
    "from WallWorld import WallWorld\n",
    "from DqnAgent import DqnAgent\n",
    "from DqnAgentNewDims import DqnAgentNewDims\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Config\n",
    "    max_steps=500_000\n",
    "\n",
    "    # WallWorld\n",
    "    render_mode=None\n",
    "    size=7\n",
    "    agentSpawn = None\n",
    "    maxSteps=200\n",
    "    stepLoss=0\n",
    "    chestSpawnCoordinates=np.array([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6],\n",
    "                                    [1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6],\n",
    "                                    [2, 0], [2, 1], [2, 2],         [2, 4], [2, 5], [2, 6]])\n",
    "    wallCoordinates=      np.array([[3, 0], [3, 1], [3, 2],         [3, 4], [3, 5], [3, 6],])\n",
    "    agentSpawnCoordinates=np.array([[4, 0],                                         [4, 6],\n",
    "                                    [5, 0],                                         [5, 6],\n",
    "                                    [6, 0],                                         [6, 6]])\n",
    "    randomWalls=0\n",
    "    redChestCoordinates=None\n",
    "    greenChestCoordinates=None\n",
    "    keyCoordinates=None\n",
    "    randomredChests=1\n",
    "    randomgreenChests=1\n",
    "    randomkeys=0\n",
    "    redChestReward=-1 # Don't change this\n",
    "    greenChestReward=1 # Don't change this\n",
    "    # Explaination:\n",
    "    # terminated with 1 reward ---> green chest: 1\n",
    "    # terminated with 0 reward ---> red chest:  -1\n",
    "    # truncated                 --> no chest:    0\n",
    "    \n",
    "    # Agent\n",
    "    batch_size=64\n",
    "    lr=0.001\n",
    "    gamma=0.95\n",
    "    epsilon_start=1\n",
    "    epsilon_min=0.05\n",
    "    epsilon_decay=200_000 # 50_000 at 3000 episodes\n",
    "    tau=0.0005 # Was 0.005\n",
    "    replayBuffer=100_000\n",
    "\n",
    "    env = WallWorld(render_mode=None,\n",
    "                    size=size, agentSpawn=agentSpawn,\n",
    "                    stepLoss=stepLoss, maxSteps=maxSteps,\n",
    "                    wallCoordinates=wallCoordinates,\n",
    "                    randomWalls=randomWalls,\n",
    "                    redChestCoordinates=redChestCoordinates,\n",
    "                    greenChestCoordinates=greenChestCoordinates,\n",
    "                    keyCoordinates=keyCoordinates,\n",
    "                    redChestReward=redChestReward,\n",
    "                    greenChestReward=greenChestReward,\n",
    "                    randomredChests=randomredChests,\n",
    "                    randomgreenChests=randomgreenChests,\n",
    "                    randomkeys=randomkeys,\n",
    "                    agentSpawnCoordinates=agentSpawnCoordinates,\n",
    "                    chestSpawnCoordinates=chestSpawnCoordinates,\n",
    "                    newDims=True)\n",
    "    observation, _ = env.reset()\n",
    "    agent = DqnAgentNewDims(env.action_space, observation,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        gamma=gamma,\n",
    "        epsilon_start=epsilon_start,\n",
    "        epsilon_min=epsilon_min,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "        tau=tau,\n",
    "        replayBuffer=replayBuffer)\n",
    "    \n",
    "    maxSteps = 30 #????\n",
    "    show_env = WallWorld(render_mode=\"human\",\n",
    "                    size=size, agentSpawn=agentSpawn,\n",
    "                    stepLoss=stepLoss, maxSteps=maxSteps,\n",
    "                    wallCoordinates=wallCoordinates,\n",
    "                    randomWalls=randomWalls,\n",
    "                    redChestCoordinates=redChestCoordinates,\n",
    "                    greenChestCoordinates=greenChestCoordinates,\n",
    "                    keyCoordinates=keyCoordinates,\n",
    "                    redChestReward=redChestReward,\n",
    "                    greenChestReward=greenChestReward,\n",
    "                    randomredChests=randomredChests,\n",
    "                    randomgreenChests=randomgreenChests,\n",
    "                    randomkeys=randomkeys,\n",
    "                    agentSpawnCoordinates=agentSpawnCoordinates,\n",
    "                    chestSpawnCoordinates=chestSpawnCoordinates,\n",
    "                    newDims=True)\n",
    "\n",
    "    modelNames = [\"r00_g10_1500k\",\n",
    "              \"r01_g10_1500k\",\n",
    "              \"r02_g10_1500k\",\n",
    "              \"r03_g10_1500k\",\n",
    "              \"r04_g10_1500k\",\n",
    "              \"r05_g10_1500k\",\n",
    "              \"r06_g10_1500k\",\n",
    "              \"r07_g10_1500k\",\n",
    "              \"r08_g10_1500k\",\n",
    "              \"r09_g10_1500k\",\n",
    "              \"r10_g00_1500k\",\n",
    "              \"r10_g01_1500k\",\n",
    "              \"r10_g02_1500k\",\n",
    "              \"r10_g03_1500k\",\n",
    "              \"r10_g04_1500k\",\n",
    "              \"r10_g05_1500k\",\n",
    "              \"r10_g06_1500k\",\n",
    "              \"r10_g07_1500k\",\n",
    "              \"r10_g08_1500k\",\n",
    "              \"r10_g09_1500k\",\n",
    "              \"r10_g10_1500k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: C:/Projects/public/XAI_Master/models/r10_g01_3000k.pth\n",
      "Right: 0.593, Down: 0.637, Left: 0.667, Up: 0.669, "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFMklEQVR4nO3bMW7DQBAEQZ7B/395nXWoiMbKUtULBjiAjQ14ZmYuALiu62d7AADvQxQAiCgAEFEAIKIAQEQBgIgCABEFAHJvD4BXzjnbEx7nf1HemUsBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADIvT0AXpmZ7QnwVVwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACD39oBN5zrbEx4312xPeNQ5H/hG81lvxGdxKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGA3NsDNs3ZXvAHZnsA8J+5FACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBA7u0Bq2a2F8BHOGd7wfO+9fPgUgAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAOTMz2yMAeA8uBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA8gvilRwFzW/xegAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#agent.load_model_weights(f\"C:/Projects/public/XAI_Master/models/r10_g10_1500k.pth\")\n",
    "#agent.load_model_weights(f\"C:/Projects/public/XAI_Master/models/newDimsModel_3000k.pth\")\n",
    "agent.load_model_weights(f\"C:/Projects/public/XAI_Master/models/r10_g01_3000k.pth\")\n",
    "\n",
    "\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "actions = agent.predict(state)\n",
    "\n",
    "def plotState(state):\n",
    "    image = state.squeeze().numpy().transpose((2, 1, 0))\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def printActions(actions):\n",
    "    action_to_direction = {0: 'Right', 1: 'Down', 2: 'Left', 3: 'Up'}\n",
    "    for i, action in enumerate(actions[0]):\n",
    "        print(f\"{action_to_direction[i]}: {action:.3f}\", end=\", \")\n",
    "\n",
    "printActions(actions)\n",
    "plotState(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent.policy_net.conv3: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "actions: tensor([[0.5926, 0.6365, 0.6671, 0.6686]])\n",
      "state.shape: torch.Size([1, 3, 7, 7])\n",
      "state.shape: torch.Size([1, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFMklEQVR4nO3bMW7DQBAEQZ7B/395nXWoiMbKUtULBjiAjQ14ZmYuALiu62d7AADvQxQAiCgAEFEAIKIAQEQBgIgCABEFAHJvD4BXzjnbEx7nf1HemUsBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADIvT0AXpmZ7QnwVVwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACD39oBN5zrbEx4312xPeNQ5H/hG81lvxGdxKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGA3NsDNs3ZXvAHZnsA8J+5FACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBA7u0Bq2a2F8BHOGd7wfO+9fPgUgAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAOTMz2yMAeA8uBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA8gvilRwFzW/xegAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grayscale_cam.shape: (1, 7, 7)\n",
      "original_img.shape: (7, 7, 3)\n",
      "grayscale_cam.shape: (7, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputs: tensor([[0.6363, 0.6582, 0.7108, 0.6870]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# implement pytorch_grad_cam using this layer\n",
    "print(f\"agent.policy_net.conv3: {agent.policy_net.conv3}\")\n",
    "print(f\"actions: {actions}\")\n",
    "print(f\"state.shape: {state.shape}\")\n",
    "\n",
    "target_layer = agent.policy_net.conv3\n",
    "\n",
    "\n",
    "# tutorial\n",
    "\"\"\"\n",
    "model = resnet50(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]\n",
    "input_tensor = # Create an input tensor image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# We have to specify the target we want to generate the CAM for.\n",
    "targets = [ClassifierOutputTarget(281)]\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images.\n",
    "with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "  # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "  grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "  # In this example grayscale_cam has only one image in the batch:\n",
    "  grayscale_cam = grayscale_cam[0, :]\n",
    "  visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "  # You can also get the model outputs without having to redo inference\n",
    "  model_outputs = cam.outputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Assuming you have the agent, state and actions defined as shown in your output\n",
    "\n",
    "# First, we need to ensure the state is in the right format\n",
    "# Your state shape is [1, 7, 7, 3] (batch, height, width, channels)\n",
    "# But PyTorch typically expects [batch, channels, height, width]\n",
    "#state_tensor = state.permute(0, 1, 3, 2)  # Rearrange to [1, 3, 7, 7]\n",
    "state_tensor = state.permute(0, 1, 3, 2)\n",
    "\n",
    "print(f\"state.shape: {state_tensor.shape}\")\n",
    "\n",
    "# Define a target class for GradCAM\n",
    "# In RL, we're typically interested in the action that was chosen\n",
    "# Let's assume we want to visualize the gradients for the action with highest probability\n",
    "action_idx = torch.argmax(actions, dim=1).item()\n",
    "targets = [ClassifierOutputTarget(action_idx)]\n",
    "\n",
    "plotState(state)\n",
    "\n",
    "# Create a GradCAM object with the agent's policy network and target layer\n",
    "with GradCAM(model=agent.policy_net, target_layers=[target_layer]) as cam:\n",
    "    # Generate the CAM\n",
    "    grayscale_cam = cam(input_tensor=state_tensor, targets=targets)\n",
    "    \n",
    "    print(f\"grayscale_cam.shape: {grayscale_cam.shape}\")\n",
    "    # Since we have only one image in the batch\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    # Convert the state to a numpy array for visualization\n",
    "    # First, convert it back to the format [height, width, channels]\n",
    "    original_img = state_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Normalize the image for visualization if it's not already in [0, 1] range\n",
    "    if original_img.max() > 1.0:\n",
    "        original_img = original_img / 255.0\n",
    "\n",
    "    print(f\"original_img.shape: {original_img.shape}\")\n",
    "    print(f\"grayscale_cam.shape: {grayscale_cam.shape}\")\n",
    "    #plt.imshow(original_img, interpolation='nearest')\n",
    "    plt.imshow(grayscale_cam, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create the CAM visualization\n",
    "    visualization = show_cam_on_image(original_img, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    # Save or display the visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(visualization)\n",
    "    plt.title(f\"GradCAM for action {action_idx}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # You can also get the model outputs without redoing inference\n",
    "    model_outputs = cam.outputs\n",
    "    print(f\"Model outputs: {model_outputs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
