{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Chunking\n",
    "**1. Create a chunker that detects noun-phrases (NPs) and lists the NPs in the text below.**\n",
    "\n",
    "- Both [NLTK](https://www.nltk.org/book/ch07.html) and [spaCy](https://spacy.io/api/matcher) supports chunking\n",
    "- Look up RegEx parsing for NLTK and the document object for spaCy.\n",
    "- Make use of what you've learned about tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The language model', 'the next word', 'It', 'a very nice word']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "text = \"The language model predicted the next word. It was a very nice word!\"\n",
    "# TODO: set up a pos tagger and a chunker.\n",
    "\n",
    "# set up a pos tagger\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def getListOfNPs(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    return noun_phrases\n",
    "    \n",
    "print(getListOfNPs(text, nlp))\n",
    "# Output: a list of all tokens, grouped as noun-phrases where applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Modify the chunker to handle verb-phases (VPs) as well.**\n",
    "- This can be done by using a RegEx parser in NLTK or using a spaCy Matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set up grammars to chunk VPs\n",
    "\n",
    "grammar = \"\"\"\n",
    "    VP: {MYGRAMMAR}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Verb-phrases (VPs) can be defined by many different grammatical rules. Give four examples.**\n",
    "- Hint: Context-Free Grammars, chapter 8 in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple verb phrase, example: \"runs\"\n",
    "\n",
    "Verb with direct object (transitive verb), example: \"eats an apple\"\n",
    "\n",
    "\n",
    "Verb with indirect and direct object, example: \"gave her a gift\"\n",
    "\n",
    "Verb with adverbial modifiers, example: \"runs quickly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. After these applications, do you find chunking to be beneficial in the context of language modeling and next-word prediction? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may enhance language modeling and next-word prediction by giving it more structural and semantic context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Use spaCy to inspect/visualise the dependency tree of the text provided below.**\n",
    "- Optional addition: visualize the dependencies as a graph using `networkx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det model NOUN []\n",
      "language compound model NOUN []\n",
      "model nsubj predicted VERB [The, language]\n",
      "predicted ROOT predicted VERB [model, word]\n",
      "the det word NOUN []\n",
      "next amod word NOUN []\n",
      "word dobj predicted VERB [the, next]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c50085a2d86749a3b576b1c699e0ba86-0\" class=\"displacy\" width=\"750\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">model</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">predicted</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">next</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">word</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c50085a2d86749a3b576b1c699e0ba86-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,52.0 245.0,52.0 245.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c50085a2d86749a3b576b1c699e0ba86-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c50085a2d86749a3b576b1c699e0ba86-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,102.0 240.0,102.0 240.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c50085a2d86749a3b576b1c699e0ba86-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,154.0 L162,142.0 178,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c50085a2d86749a3b576b1c699e0ba86-0-2\" stroke-width=\"2px\" d=\"M270,152.0 C270,102.0 340.0,102.0 340.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c50085a2d86749a3b576b1c699e0ba86-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,154.0 L262,142.0 278,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c50085a2d86749a3b576b1c699e0ba86-0-3\" stroke-width=\"2px\" d=\"M470,152.0 C470,52.0 645.0,52.0 645.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c50085a2d86749a3b576b1c699e0ba86-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,154.0 L462,142.0 478,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c50085a2d86749a3b576b1c699e0ba86-0-4\" stroke-width=\"2px\" d=\"M570,152.0 C570,102.0 640.0,102.0 640.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c50085a2d86749a3b576b1c699e0ba86-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,154.0 L562,142.0 578,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c50085a2d86749a3b576b1c699e0ba86-0-5\" stroke-width=\"2px\" d=\"M370,152.0 C370,2.0 650.0,2.0 650.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c50085a2d86749a3b576b1c699e0ba86-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M650.0,154.0 L658.0,142.0 642.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "text = \"The language model predicted the next word\"\n",
    "# TODO: use spacy and displacy to visualize the dependency tree\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])\n",
    "\n",
    "# Visualize the dependency tree\n",
    "displacy_image = displacy.render(doc, style=\"dep\", jupyter=False, options={'distance': 100})\n",
    "display(SVG(displacy_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the root of the sentence? Attempt to spot it yourself, but the answer should be done by code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicted', 'VERB')\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement a function to find the root of the document\n",
    "# Return both the word and its POS tag\n",
    "\n",
    "def getSentenceRoot(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            return token.text, token.pos_\n",
    "\n",
    "print(getSentenceRoot(text, nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the subject and object of a sentence. Print the results for the sentence above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('model', 'NOUN')], [('word', 'NOUN')])\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement a function to find the subjects + objects in the document\n",
    "\n",
    "def getSubjectsAndObjects(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    subjects = [(token.text, token.pos_) for token in doc if \"subj\" in token.dep_]\n",
    "    objects = [(token.text, token.pos_) for token in doc if \"obj\" in token.dep_]\n",
    "    return subjects, objects\n",
    "\n",
    "print(getSubjectsAndObjects(text, nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. How would you use the relationships extracted from dependency parsing in language modeling contexts?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would try to use it to find the meaning of words that can be both noun ar verb for example. Also I would use to to help a system determine if there are typing errors in a text by adressing if the grammatical structure makes sense, possible exchanging words with a mask such that the full sentence is coherrent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Use Wordnet (from NLTK) and create a function to get all synonyms of a word of your choice. Try with \"language\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "speech\n",
      "lyric\n",
      "linguistic_process\n",
      "language\n",
      "terminology\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# nltk.download('wordnet')\n",
    "# TODO: find synonyms\n",
    "\n",
    "def getListOfSynonyms(word):\n",
    "    synonyms = wn.synsets(word)\n",
    "    return [synonym.lemmas()[0].name() for synonym in synonyms]\n",
    "\n",
    "getListOfSynonyms = getListOfSynonyms(\"language\")\n",
    "for synonym in getListOfSynonyms:\n",
    "    print(synonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. From the same word you chose, extract an additional 4 or more features from wordnet (such as hyponyms). Describe each category briefly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antonyms []\n",
      "hyponyms ['artificial_language', 'barrage', 'dead_language', 'indigenous_language', 'lingua_franca', 'metalanguage', 'native_language', 'natural_language', 'object_language', 'sign_language', 'slanguage', 'source_language', 'string_of_words', 'superstrate', 'usage', 'words', 'conversation', 'dictation', 'discussion', 'idiolect', 'monologue', 'non-standard_speech', 'pronunciation', 'saying', 'soliloquy', 'spell', 'words', 'love_lyric', 'reading', 'markup_language', 'toponymy']\n",
      "hypernyms ['communication', 'auditory_communication', 'text', 'higher_cognitive_process', 'faculty', 'word']\n",
      "meronyms ['lexis', 'vocabulary']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Antonyms:\n",
    "Words that have the opposite meaning of the chosen word.\n",
    "Hyponyms:\n",
    "More specific terms that fall under the category of the chosen word.\n",
    "Hypernyms:\n",
    "Broader terms that encompass the chosen word within a larger category.\n",
    "Meronyms:\n",
    "Words that represent parts of or pieces of the chosen word.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: expand the function to find more features!\n",
    "\n",
    "def getListOfWordFeatures(word):\n",
    "    synsets  = wn.synsets(word)\n",
    "    antonyms = []\n",
    "    hyponyms = []\n",
    "    hypernyms = []\n",
    "    meronyms = []\n",
    "    for synset in synsets:\n",
    "        antonyms.extend([antonym.lemmas()[0].name() for antonym in synset.lemmas()[0].antonyms()])\n",
    "        hyponyms.extend([hyponym.lemmas()[0].name() for hyponym in synset.hyponyms()])\n",
    "        hypernyms.extend([hypernym.lemmas()[0].name() for hypernym in synset.hypernyms()])\n",
    "        meronyms.extend([meronym.lemmas()[0].name() for meronym in synset.part_meronyms()])\n",
    "    return {\n",
    "        \"antonyms\": antonyms,\n",
    "        \"hyponyms\": hyponyms,\n",
    "        \"hypernyms\": hypernyms,\n",
    "        \"meronyms\": meronyms\n",
    "    }\n",
    "\n",
    "word_features = getListOfWordFeatures(\"language\")\n",
    "for feature, values in word_features.items():\n",
    "    print(feature, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Exercise - A sentiment classifier\n",
    "- A rule-based approach with SentiWordNet + A machine learning classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. There are several steps required to build a classifier or any sort of machine learning application for textual data. For data including (INPUT_TEXT, LABEL), list the typical pipeline for classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical pipeline for classifying textual data involves first preprocessing the text. Like tokenization, normalization, and removing stopwords\n",
    "Then feature extraction such as TF-IDF or word embeddings to convert text into a machine learning friendly format.\n",
    "Then the processed data is fed into a classification model that is trained on the input features to predict the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Before developing a classifier, having a baseline is very useful. Build a baseline model for sentiment classification using SentiWordNet.**\n",
    "- How you decide to aggregate sentiment is up to you. Explain your approach.\n",
    "- It should report the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5312093733737563 1\n",
      "0.20181322226037884 0\n",
      "0.29421497216298875 0\n",
      "0.46879062662624377 0\n",
      "0.5 1\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "import math\n",
    "# nltk.download('sentiwordnet')\n",
    "\n",
    "# TODO: implement a function to get the sentiment of a text\n",
    "# Must use the sentiwordnet lexicon\n",
    "\n",
    "# I've used the sentiwordnet to get the sentiment of\n",
    "# each token in the text, and then I've summed them up.\n",
    "# The result is then passed through a sigmoid function\n",
    "# to get a value between 0 and 1, which is then returned.\n",
    "\n",
    "def getSentiment(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    total_sentiment = 0\n",
    "    for token in doc:\n",
    "        synsets = wn.synsets(token.text)\n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            sentiment = swn.senti_synset(synset.name())\n",
    "            total_sentiment += sentiment.pos_score() - sentiment.neg_score()\n",
    "    sigmoidValue = 1 / (1 + math.exp(-total_sentiment))\n",
    "    return sigmoidValue\n",
    "\n",
    "def getAccuracy(y_true, y_pred):\n",
    "    return sum([1 for true, pred in zip(y_true, y_pred) if true == pred]) / len(y_true)\n",
    "\n",
    "# Evaluate it on the following sentences:\n",
    "sents = [\n",
    "    \"I liked it! Did you?\",\n",
    "    \"It's not bad but... Nevermind, it is.\",\n",
    "    \"It's awful\",\n",
    "    \"I don't care if you loved it - it was terrible!\",\n",
    "    \"I don't care if you hated it, I think it was awesome\"\n",
    "]\n",
    "# 0: negative, 1: positive\n",
    "y_true = [1, 0, 0, 0, 1]\n",
    "\n",
    "preds = []\n",
    "\n",
    "for sent, true in zip(sents, y_true):\n",
    "    print(getSentiment(sent, nlp), true)\n",
    "    preds.append(round(getSentiment(sent, nlp), 0))\n",
    "\n",
    "print(f\"Accuracy: {getAccuracy(y_true, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SST-2 binary sentiment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Split the training set into a training and test set. Choose a split size, and justify your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    5535\n",
      "0    4465\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11724</th>\n",
       "      <td>this listless feature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59435</th>\n",
       "      <td>close to real life</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66633</th>\n",
       "      <td>a lovely trifle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58626</th>\n",
       "      <td>not the great american comedy , but if you lik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47410</th>\n",
       "      <td>a very funny , heartwarming film</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "11724                             this listless feature       0\n",
       "59435                                close to real life       1\n",
       "66633                                   a lovely trifle       1\n",
       "58626  not the great american comedy , but if you lik...      1\n",
       "47410                  a very funny , heartwarming film       1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "train_df = dataset[\"train\"].to_pandas().drop(columns=[\"idx\"])\n",
    "train_df = train_df.sample(10000)  # a tiny subset\n",
    "print(train_df.label.value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points: (10000, 2)\n",
      "Training data points: (8500, 2)\n",
      "Testing data points: (1500, 2)\n",
      "                                             sentence  label\n",
      "0                              this listless feature       0\n",
      "25                            gorgeous color palette       1\n",
      "29  in a summer of clones , harvard man is somethi...      1\n",
      "33                            sour , bloody and mean       0\n",
      "39                                 own meager weight       0\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data\n",
    "\n",
    "# I think 15% testing data is a good amount\n",
    "\n",
    "def splitData(df, train_size=0.85):\n",
    "    train_xy = df.sample(frac=train_size)\n",
    "    test_xy = df.drop(train_xy.index)\n",
    "    return train_xy, test_xy\n",
    "\n",
    "df = train_df.copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "train_xy, test_xy = splitData(df, 0.85)\n",
    "\n",
    "print(f\"Training data points: {train_df.shape}\")\n",
    "print(f\"Training data points: {train_xy.shape}\")\n",
    "print(f\"Testing data points: {test_xy.shape}\")\n",
    "\n",
    "print(test_xy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluate your baseline model on the test set.**\n",
    "\n",
    "- Additionally: compare it against a random baseline. That is, a random guess for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this listless feature  0\n",
      "0.5621765008857981 0\n",
      "gorgeous color palette  1\n",
      "0.679178699175393 1\n",
      "in a summer of clones , harvard man is something rare and riveting : a wild ride that relies on more than special effects .  1\n",
      "0.6224593312018546 1\n",
      "sour , bloody and mean  0\n",
      "0.5 0\n",
      "own meager weight  0\n",
      "0.34864513533394575 0\n",
      "by someone who obviously knows nothing about crime  0\n",
      "0.7549149868676283 0\n",
      "gimmicks  0\n",
      "0.3775406687981454 0\n",
      "is n't that much different from many a hollywood romance  0\n",
      "0.7310585786300049 0\n",
      "a markedly inactive film , city is conversational bordering on confessional .  0\n",
      "0.40733340004593027 0\n",
      "one fantastic visual trope  1\n",
      "0.5926665999540697 1\n",
      "Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate on test set + random guess\n",
    "# Report results in terms of accuracy\n",
    "\n",
    "preds = []\n",
    "printCount = 10\n",
    "\n",
    "for index, row in test_xy.iterrows():\n",
    "    if printCount > 0:\n",
    "        print(row['sentence'], row['label'])\n",
    "        print(getSentiment(row['sentence'], nlp), row['label'])\n",
    "        printCount -= 1\n",
    "    preds.append(round(getSentiment(row['sentence'], nlp), 0))\n",
    "\n",
    "print(f\"Accuracy: {getAccuracy(y_true, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Did you beat random guess?**\n",
    "\n",
    "If not, can you think of any reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be vecause many sentences hasn't got much sentiment to them but they still have a score of 0 or 1. For example: \"this listless feature\" sentiment: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Naive Bayes and TF-IDF\n",
    "This is the final task of the lab. You will use high-level libraries to implement a TF-IDF vectorizer and train your data using a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.75      0.80       664\n",
      "           1       0.82      0.90      0.86       836\n",
      "\n",
      "    accuracy                           0.83      1500\n",
      "   macro avg       0.84      0.82      0.83      1500\n",
      "weighted avg       0.83      0.83      0.83      1500\n",
      "\n",
      "Accuracy: 0.8313333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TODO: use scikit-learn to...\n",
    "# - normalize\n",
    "# - vectorize/extract features\n",
    "# - train a classifier\n",
    "# - evaluate the classifier using `classification_report` and `accuracy`\n",
    "# \n",
    "# expect an accuracy of > 0.8\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "normalizer = Normalizer()\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "pipeline = make_pipeline(vectorizer, normalizer, classifier)\n",
    "pipeline.fit(train_xy['sentence'], train_xy['label'])\n",
    "preds = pipeline.predict(test_xy['sentence'])\n",
    "\n",
    "print(classification_report(test_xy['label'], preds))\n",
    "print(f\"Accuracy: {getAccuracy(test_xy['label'], preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional task: using a pre-trained transformer model\n",
    "If you wish to push the accuracy as far as you can, take a look at BERT-based or other pre-trained language models. As a starting point, take a look at a model already fine-tuned on the SST-2 dataset: [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "\n",
    "**Advanced:**\n",
    "\n",
    "Going beyond this, you could look into the addition of a *classification head* on top of the pooling layer of a BERT-based model. This is a common approach to fine-tuning these models on classification or regression problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
