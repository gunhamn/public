{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Given the list of pluralized words below, define your own simple word stemmer function or class,  limited to only simple rules and regex. No libraries! It should strip basic endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fli', 'deni', 'itemizat', 'sensat', 'reference', 'coloniz']\n"
     ]
    }
   ],
   "source": [
    "plurals = [\n",
    "    \"flies\",\n",
    "    \"denied\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "]\n",
    "\n",
    "# TODO: implement your own ismple stemmer\n",
    "\n",
    "def stemmer(wordArray):\n",
    "    endings = [\"es\", \"ed\", \"al\", \"ion\", \"er\"]\n",
    "    stemmedWords = []\n",
    "    for word in wordArray:\n",
    "        for ending in endings:\n",
    "            if word.endswith(ending):\n",
    "                word = word[:-len(ending)]\n",
    "        stemmedWords.append(word)\n",
    "    return stemmedWords\n",
    "\n",
    "print(stemmer(plurals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. After your initial implementation, run it on the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['friendly', 'puzzling', 'helpful']\n"
     ]
    }
   ],
   "source": [
    "new_words = [\n",
    "    \"friendly\",\n",
    "    \"puzzling\",\n",
    "    \"helpful\",\n",
    "]\n",
    "# TODO: run your stemmer on the new words\n",
    "\n",
    "print(stemmer(new_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Realizing that fixing future words manually can be problematic, use a desired NLTK stemmer and run it on all the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fli', 'deni', 'item', 'sensat', 'refer', 'colon', 'friendli', 'puzzl', 'help']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "all_words = plurals + new_words\n",
    "\n",
    "# TODO: use an nltk stemming implementation to stem `all_words`\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmedWords = [stemmer.stem(word) for word in all_words]\n",
    "print(stemmedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. There are likely a few words in the outputs above that would cause issues in real-world applications. Pick some examples, and show how they are solved with a lemmatizer. Use either spaCy or nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here! Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fly', 'deny', 'itemization', 'sensational', 'reference', 'colonizer', 'friendly', 'puzzle', 'helpful']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# TODO: basic observations on which examples are problematic with stemming + implement lemmatization with spacy/nltk\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "allWordsLemmatized = [nlp(word)[0].lemma_ for word in all_words]\n",
    "\n",
    "print(allWordsLemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/Lemmatization - Practical Example\n",
    "Using the news corpus (subset/category of the Brown corpus), perform common text normalization techniques such as stopword filtering and stemming/lemmatization. Compare the top 10 most common **words** before and after these normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5580), (',', 5188), ('.', 4030), ('of', 2849), ('and', 2146), ('to', 2116), ('a', 1993), ('in', 1893), ('for', 943), ('The', 806)]\n"
     ]
    }
   ],
   "source": [
    "# import nltk; nltk.download('brown')  # ensure we have the data\n",
    "from nltk.corpus import brown\n",
    "news = brown.words(categories='news')\n",
    "\n",
    "# TODO: find the top 10 most common words\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(news).most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('say', 464), ('year', 319), ('would', 249), ('new', 245), ('one', 233), ('two', 191), ('state', 189), ('make', 184), ('last', 180), ('president', 164)]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# TODO: find the top 10 most common words after applying text normalization techniques\n",
    "\n",
    "\"\"\"\n",
    "# remove non-alphabetic characters\n",
    "newsAlpha = [word for word in news if word.isalpha()]\n",
    "newsLemmatized = [nlp(word)[0].lemma_ for word in newsAlpha]\n",
    "print(Counter(newsLemmatized).most_common(10))\n",
    "# [('the', 6386), ('of', 2861), ('be', 2840), ('and', 2186), ('to', 2144),\n",
    "# ('a', 2130), ('in', 2020), ('for', 969), ('have', 861), ('that', 829)]\n",
    "# Runtime: 3m 14s\n",
    "\n",
    "\n",
    "news_text = ' '.join(news)\n",
    "newsLemmatized = [token.lemma_.lower() for token in nlp(news_text) if token.is_alpha]\n",
    "print(Counter(newsLemmatized).most_common(10))\n",
    "# [('the', 6390), ('of', 2864), ('be', 2853), ('and', 2190), ('to', 2155),\n",
    "# ('a', 2139), ('in', 2034), ('for', 972), ('have', 860), ('that', 843)]\n",
    "# Runtime: 8.6s\n",
    "\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply stopword filtering along with lemmatization\n",
    "news_text = ' '.join(news)\n",
    "news_normalised = [token.lemma_.lower() for token in nlp(news_text)\n",
    "                 if token.is_alpha and token.lemma_.lower() not in stop_words]\n",
    "\n",
    "print(Counter(news_normalised).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a way to measure the importance of a word in a document.\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t$ is the term (word)\n",
    "- $d$ is the document\n",
    "- $D$ is the corpus\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Implement TF-IDF using NLTKs FreqDist (no use of e.g. scikit-learn and other high-level libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes to self:\n",
    "**Term Frequency (TF)**\n",
    "How frequently a term occurs in a document.\n",
    "$$\n",
    "\\text{tf}(t, d) = \\frac{\\text{instances of term } t \\text{ in document } d}{\\text{total number of terms in document } d}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (IDF)**\n",
    "How important a term is.\n",
    "$$\n",
    "\\text{idf}(t, D) = \\log \\left( \\frac{\\text{total number of documents in corpus } D}{\\text{number of documents containing term } t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "0.05549257115579689\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "##########################################################\n",
    "# Feel free to change everything below.\n",
    "# It is merely a guide to understand the inputs/outputs\n",
    "##########################################################\n",
    "\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def tf(document: List[str], term: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the term frequency (TF) of a given term in a document.\n",
    "\n",
    "    Args:\n",
    "        document (List[str]): The document in which to calculate the term frequency.\n",
    "        term (str): The term for which to calculate the term frequency.\n",
    "\n",
    "    Returns:\n",
    "        float: The term frequency of the given term in the document.\n",
    "    \"\"\"\n",
    "    return FreqDist(document).freq(term)\n",
    "    # or\n",
    "    freq_dist = FreqDist(document)\n",
    "    instancesOfTerm = freq_dist.get(term)\n",
    "    totalWords = freq_dist.N()\n",
    "    return instancesOfTerm / totalWords\n",
    "    \n",
    "\n",
    "print(news)\n",
    "print(tf(news, \"the\"))\n",
    "\n",
    "############ TODO ############\n",
    "def idf(documents: List[List[str]], term: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) of a term in a collection of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (List[List[str]]): A list of documents, where each document is represented as a list of strings.\n",
    "        term (str): The term for which IDF is calculated.\n",
    "\n",
    "    Returns:\n",
    "        float: The IDF value of the term.\n",
    "    \"\"\"\n",
    "    # Convert each document to a set of words for faster membership checking\n",
    "    documents_sets = [set(doc) for doc in documents]\n",
    "\n",
    "    # Count documents containing the term using set for faster lookup\n",
    "    nt = sum(1 for doc_set in documents_sets if term in doc_set)\n",
    "\n",
    "    # Total number of documents\n",
    "    N = len(documents)\n",
    "\n",
    "    if nt == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log(N / nt)\n",
    "\n",
    "\n",
    "############ TODO ############\n",
    "def tf_idf(\n",
    "    all_documents: List[List[str]],\n",
    "    document: List[str],\n",
    "    term: str,\n",
    ") -> float:\n",
    "    return tf(document, term) * idf(all_documents, term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. With your TF-IDF function in place, calculate the TF-IDF for the following words in the first document of the news articles found in the Brown corpus: \n",
    "\n",
    "- *the*\n",
    "- *nevertheless*\n",
    "- *highway*\n",
    "- *election*\n",
    "\n",
    "Perform any preprocessing steps you deem necessary. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for 'the': 0.0\n",
      "TF-IDF for 'nevertheless': 0.0\n",
      "TF-IDF for 'highway': 0.003593546849130443\n",
      "TF-IDF for 'election': 0.009251767873746217\n"
     ]
    }
   ],
   "source": [
    "fileids = brown.fileids(categories='news')\n",
    "first_doc = list(brown.words(fileids[0]))\n",
    "all_docs = [list(brown.words(fileid)) for fileid in fileids]\n",
    "\n",
    "# TODO: preprocess and calculate tf-idf scores.\n",
    "\n",
    "terms = [\"the\", \"nevertheless\", \"highway\", \"election\"]\n",
    "\n",
    "for term in terms:\n",
    "    print(f\"TF-IDF for '{term}': {tf_idf(all_docs, first_doc, term)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: \"the\" and \"nevertheless\" both yeilded a value of zero becuase \"the\" was in all documents (log(1)) and \"nevertheless\" was in no documents (0 instances). In the first document \"election\" seemed to be a hotter topic than highway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. While TF-IDF is primarily used for information retrieval and text mining, reflect on how TF-IDF could be used in a language modeling context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be used to help the model's understanding and generation of text by prioritizing words that are more relevant and distinctive to the topics being modeled. This could lead to better performance, especially in tasks where its important to understand the nuances in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. You were previously introduced to word representations. TF-IDF can be considered one. What are some differences between the TF-IDF output and one that is computed once from a vocabulary (e.g. one-hot encoding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF's can give two different words the same score and is therefore not a unique identifyer. One-hot encoding says nothing about the word but is purely an identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Practical Example\n",
    "You will again be looking at specific words for a document, but this time weighted by their TF-IDF scores. Ideally, the scoring should be able to retrieve representative words for this document in context of its document collection or category.\n",
    "\n",
    "You will do the following:\n",
    "- Select a category from the Reuters (news) corpus\n",
    "- Perform preprocessing\n",
    "- Calculate TF-IDF scores\n",
    "- Find the top 5 words for a subset of documents in your collection (e.g. 5, 10, ..)\n",
    "- Inspect whether these words make sense for a given document, and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available categories: ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "Number of coconut texts: 6\n",
      "Most common words:\n",
      " [('say', 24), ('coconut', 16), ('romero', 11), ('ec', 8), ('copra', 7), ('would', 6), ('oil', 6), ('year', 6), ('production', 6), ('country', 5), ('export', 5), ('meal', 5), ('level', 5), ('aflatoxin', 5), ('land', 5)]\n",
      "TF-IDF for 'say': 0.023909490404412644\n",
      "TF-IDF for 'coconut': 0.0\n",
      "TF-IDF for 'romero': 0.010958516435355795\n",
      "TF-IDF for 'ec': 0.021594344740405107\n",
      "TF-IDF for 'copra': 0.01192145027990078\n",
      "TF-IDF for 'would': 0.010218385954200668\n",
      "TF-IDF for 'oil': 0.010218385954200668\n",
      "TF-IDF for 'year': 0.010218385954200668\n",
      "TF-IDF for 'production': 0.026414144509504498\n",
      "TF-IDF for 'country': 0.01349646546275319\n",
      "TF-IDF for 'export': 0.0049811438342526335\n",
      "TF-IDF for 'meal': 0.01349646546275319\n",
      "TF-IDF for 'level': 0.022011787091253746\n",
      "TF-IDF for 'aflatoxin': 0.022011787091253746\n",
      "TF-IDF for 'land': 0.022011787091253746\n"
     ]
    }
   ],
   "source": [
    "# import nltk; nltk.download(\"reuters\")\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "categories = reuters.categories()\n",
    "print(\"Available categories:\", categories)\n",
    "\n",
    "coconutTexts = reuters.fileids(categories=\"coconut\")\n",
    "print(\"Number of coconut texts:\", len(coconutTexts))\n",
    "\n",
    "coconutTexts_normalised = []\n",
    "for i in range(len(coconutTexts)):\n",
    "    textString = ' '.join(reuters.words(coconutTexts[i]))\n",
    "    coconutTexts_normalised.append([token.lemma_.lower() for token in nlp(textString)\n",
    "                 if token.is_alpha and token.lemma_.lower() not in stop_words])\n",
    "\n",
    "print(\"Most common words:\\n\", Counter(coconutTexts_normalised[0]).most_common(15))\n",
    "\n",
    "top15words = [term for term, _ in Counter(coconutTexts_normalised[0]).most_common(15)]\n",
    "\n",
    "for word in top15words:\n",
    "    print(f\"TF-IDF for '{word}': {tf_idf(coconutTexts_normalised, coconutTexts_normalised[0], word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Sadly, \"coconut\" got 0... but, of course it's because all articles are about coconut so it per definition not relevant when we know that the category already is coconuts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly describe your understanding of POS tagging and its possible use-cases in context of text generation applications/language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tags words into grammatical categories, making it easier to model the complete sentence. This helps understanding the context of words and can clarify the meaning of words that can mean many things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train a UnigramTagger (NLTK) using the Brown corpus. \n",
    "Hint: the taggers in nltk require a list of sentences containing tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a unigram tagger on the brown corpus\n",
    "from nltk import UnigramTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "tagger = UnigramTagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use this tagger to tag the text given below. Print out the POS tags for all variants of \"justify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine (None)\n",
      "a (AT)\n",
      "situation (NN)\n",
      "where (WRB)\n",
      "you (PPSS)\n",
      "have (HV)\n",
      "to (TO)\n",
      "explain (VB)\n",
      "why (WRB)\n",
      "you (PPSS)\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Imagine a situation where you have to explain why you did something â€“ that's when you justify your actions. So, let's say you made a decision; you, as the justifier, need to give good reasons (justifications) for your choice. You might use justifying words to make your point clear and reasonable. Justifying can be a bit like saying, \"Here's why I did what I did.\" When you justify things, you're basically providing the why behind your actions. So, being a good justifier involves carefully explaining, giving reasons, and making sure others understand your choices\n",
    "\"\"\"\n",
    "\n",
    "# TODO: use your trained tagger\n",
    "\n",
    "text_tokens = text.split()\n",
    "tagged_text = tagger.tag(text_tokens)\n",
    "for word, tag in tagged_text[:10]:\n",
    "    print(f\"{word} ({tag})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Your results may be disappointing. Repeat the same task as above using both the default NLTK pos-tagger and with spaCy. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine (VB)\n",
      "a (DT)\n",
      "situation (NN)\n",
      "where (WRB)\n",
      "you (PRP)\n",
      "have (VBP)\n",
      "to (TO)\n",
      "explain (VB)\n",
      "why (WRB)\n",
      "you (PRP)\n"
     ]
    }
   ],
   "source": [
    "# TODO: use the default NLTK tagger\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_text = nltk.pos_tag(text_tokens)\n",
    "for word, tag in tagged_text[:10]:\n",
    "    print(f\"{word} ({tag})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (SPACE)\n",
      "Imagine (VERB)\n",
      "a (DET)\n",
      "situation (NOUN)\n",
      "where (SCONJ)\n",
      "you (PRON)\n",
      "have (VERB)\n",
      "to (PART)\n",
      "explain (VERB)\n",
      "why (SCONJ)\n"
     ]
    }
   ],
   "source": [
    "# TODO: use spacy to fetch pos tags from the document\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc[:10]:\n",
    "    print(f\"{token.text} ({token.pos_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finally, explore more features of the what the spaCy *document* includes related to topics covered in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_context', '_get_array_attrs', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'from_json', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n",
      "situation (NOUN)\n",
      "actions (NOUN)\n",
      "decision (NOUN)\n",
      "justifier (NOUN)\n",
      "reasons (NOUN)\n",
      "justifications (NOUN)\n",
      "choice (NOUN)\n",
      "words (NOUN)\n",
      "point (NOUN)\n",
      "Justifying (NOUN)\n",
      "bit (NOUN)\n",
      "things (NOUN)\n",
      "actions (NOUN)\n",
      "justifier (NOUN)\n",
      "reasons (NOUN)\n",
      "others (NOUN)\n",
      "choices (NOUN)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "print(dir(doc))\n",
    "\n",
    "# print nouns\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(f\"{token.text} ({token.pos_})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
