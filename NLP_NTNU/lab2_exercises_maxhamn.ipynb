{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Kochmar mentions several steps required in a typical NLP pipeline, one of them being *Split into words*. Why is this step necessary? Why can we not just feed the text as it is into a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ml-model cannot take raw text as input, only numbers. Therefore, we need to process the data in a way that lets the model interpret the data. We could split the text into characters but this would be a less meaningful way to interpret the data compared to words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simply splitting on \"words\" (i.e. whitespace) is rarely enough. Consider the sentence below (\"That U.S.A. poster-print costs $12.40...\") and name some problems that arise from splitting on whitespace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be words that consist of other words, like \"poster-print\". Here, \"(dollar)12.40...\" would be one word, when splitting it into \"$\", \"12.40\" and \"...\" could perhaps be a more sensible way to interpret its meaning. Also, \"costs\" is very closely related to the meaning of \"cost\", the only difference being that it refers to a singular noun. Not dealing with these types of words would mean that the model in practice would have to learn the same word twice.\n",
    "\n",
    "Another challenge is how different letters are used in coding languages, which can be interpreted as having another meaning for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you wish, experiment with implementing different rules for tokenization. You will see that the \"ruleset\" quickly grows if you want to account for all types of edge cases...\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "def your_rulebased_tokenizer(sentence):\n",
    "    tokens = []\n",
    "    current_token = \"\"\n",
    "    for char in sentence:\n",
    "        if char == \" \":\n",
    "            tokens.append(current_token)   \n",
    "            current_token = \"\"\n",
    "        else:\n",
    "            current_token += char\n",
    "    return tokens\n",
    "\n",
    "your_rulebased_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has several tokenizers implemented, such as a specific one for Twitter data. Below, indicated by the `TODO`-tag, you should find and import various tokenizers and add them to the list of tokenizers:\n",
    "\n",
    "`tokenizers = [tokenizer1, tokenizer2, ..., tokenizerN]`\n",
    "\n",
    "Tokenize the sentence with at least three different tokenizers supplied by NLTK and comment on your findings. You will find the documentation for NLTK's tokenizers [here](https://www.nltk.org/_modules/nltk/tokenize.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyWhitespaceTokenizer (5 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "WordPunctTokenizer (16 tokens)\n",
      "['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '...']\n",
      "\n",
      "TreebankWordTokenizer (7 tokens)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# this is the base class of tokenizers in nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
    "\n",
    "# this is just a simple example of how a tokenizer can be implemented\n",
    "class MyWhitespaceTokenizer(TokenizerI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "sentence = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "# ************************************************************\n",
    "# TODO: import and add the tokenizers you want to try out here\n",
    "# ************************************************************\n",
    "tokenizers = [\n",
    "    MyWhitespaceTokenizer(),\n",
    "    WordPunctTokenizer(),\n",
    "    TreebankWordTokenizer()\n",
    "]\n",
    "\n",
    "# Leave this function as-is\n",
    "def tokenize(tokenizers: List[TokenizerI], sentence: str) -> None:\n",
    "    for tokenizer in tokenizers:\n",
    "        assert isinstance(tokenizer, TokenizerI)\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        print(f\"{tokenizer.__class__.__name__} ({len(tokenized)} tokens)\\n{tokenized}\\n\")\n",
    "\n",
    "\n",
    "tokenize(tokenizers, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "The WordPunctTokenizer split the sentence up into 16 tokens while the TreebankWordTokenizer used 7 tokens. I feel like the WordPunctTokenizer splits it too much since word like U.S.A. and poster-print lose their meaning. I would prefer the third option to the first since it makes sense to let \"...\" be a token that can be interpreted in context of the whole sentence, not just the price. Separating $ from 12.40 also makes sense since it identifies the number and the $ carries a meaning of its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language modeling\n",
    "We have now studied the bigger models like BERT and GPT-based language models. A simpler language model, however, can implemented using n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is an n-gram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of n words. 2-grams would be [a, collection], [collection, of] and so on.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use NLTK to print out bigrams and trigrams for the given sentence below. Your function should support any number of N.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['That', 'U.S.A.'],\n",
       " ['U.S.A.', 'poster-print'],\n",
       " ['poster-print', 'costs'],\n",
       " ['costs', '$'],\n",
       " ['$', '12.40'],\n",
       " ['12.40', '...'],\n",
       " ['...', 'I'],\n",
       " ['I', \"'d\"],\n",
       " [\"'d\", 'pay'],\n",
       " ['pay', '$'],\n",
       " ['$', '5.00'],\n",
       " ['5.00', 'for'],\n",
       " ['for', 'it'],\n",
       " ['it', '.']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"That U.S.A. poster-print costs $12.40... I'd pay $5.00 for it.\"\n",
    "\n",
    "# ************************************\n",
    "# TODO: your implementation of n-grams\n",
    "# ************************************\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def nGrams(n, tokenizer, sentence):\n",
    "    nGrams = []\n",
    "    tokenizedSentence = tokenizer.tokenize(sentence)\n",
    "    for i in range(len(tokenizedSentence) - n + 1):\n",
    "        nGrams.append(tokenizedSentence[i:i+n])\n",
    "    return nGrams\n",
    "\n",
    "\n",
    "nGrams(2, TreebankWordTokenizer(), sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Based on your intuition for language modeling, how can n-grams be used for word predictions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word prediction is all about letting the code understand the broader context. Predicting the next word besed on solely one word will result in very generic sentences that I assume won't make much sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. NLTK includes the `FreqDist` class, which produces the frequency distribution of words in a sentence. Use it to print out the two most common words in the text below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that 4\n",
      "is 3\n",
      "That 1\n",
      "not. 1\n",
      "Is 1\n",
      "it? 1\n",
      "It 1\n",
      "is. 1\n",
      "You 1\n",
      "sure? 1\n",
      "Surely 1\n",
      "it 1\n",
      "is! 1\n"
     ]
    }
   ],
   "source": [
    "text = \"That that is is that that is not. Is that it? It is. You sure? Surely it is!\"\n",
    "\n",
    "# TODO\n",
    "from nltk import FreqDist\n",
    "\n",
    "for word, frequency in FreqDist(text.split()).most_common():\n",
    "    print(word, frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Use your n-gram function from question 2.2 to print out the most common trigram of the text in question 2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('That', 'that', 'is'): 1\n",
      "('that', 'is', 'is'): 1\n",
      "('is', 'is', 'that'): 1\n",
      "('is', 'that', 'that'): 1\n",
      "('that', 'that', 'is'): 1\n",
      "('that', 'is', 'not.'): 1\n",
      "('is', 'not.', 'Is'): 1\n",
      "('not.', 'Is', 'that'): 1\n",
      "('Is', 'that', 'it'): 1\n",
      "('that', 'it', '?'): 1\n",
      "('it', '?', 'It'): 1\n",
      "('?', 'It', 'is.'): 1\n",
      "('It', 'is.', 'You'): 1\n",
      "('is.', 'You', 'sure'): 1\n",
      "('You', 'sure', '?'): 1\n",
      "('sure', '?', 'Surely'): 1\n",
      "('?', 'Surely', 'it'): 1\n",
      "('Surely', 'it', 'is'): 1\n",
      "('it', 'is', '!'): 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "trigrams = nGrams(3, TreebankWordTokenizer(), text)\n",
    "\n",
    "nGrams_tuples = [tuple(ngram) for ngram in trigrams]\n",
    "\n",
    "# Count the frequency of each n-gram\n",
    "nGrams_freq = Counter(nGrams_tuples)\n",
    "\n",
    "# Print the frequency of each n-gram\n",
    "for ngram, frequency in nGrams_freq.items():\n",
    "    print(f\"{ngram}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. You may have discovered that you would need to implement some form of preprocessing to get the correct answer to the previous tasks. Preprocessing/cleaning/normalization is often necessary for the desired results. If you were to process the text of a news site or blog post, can you think of some preprocessing steps that would be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating symbols from words, such as dots, exclamaition marks, dollar signs. Removing capitol letters from the first words of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Representations\n",
    "For more information on word representations, consult the lab description file and course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe the main differences between bag-of-words and one-hot encoding through examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary: {The, cat, is, cute, dog, happy}\n",
    "\n",
    "\n",
    "#### Bag of Words (BoW):\n",
    "\n",
    "Document 1: \"The cat is cute.\" - BoW Vector: [1, 1, 1, 1, 0, 0]\n",
    "\n",
    "Document 2: \"The dog is happy.\" - BoW Vector: [1, 0, 1, 0, 1, 1]\n",
    "\n",
    "\n",
    "#### One-hot encoding:\n",
    "\n",
    "Document 1: \"The cat is cute.\"\n",
    "| Word  | Vector         |\n",
    "|-------|----------------|\n",
    "| The   | [1, 0, 0, 0, 0, 0] |\n",
    "| cat   | [0, 1, 0, 0, 0, 0] |\n",
    "| is    | [0, 0, 1, 0, 0, 0] |\n",
    "| cute  | [0, 0, 0, 1, 0, 0] |\n",
    "\n",
    "Document 2: \"The dog is happy.\"\n",
    "| Word  | Vector         |\n",
    "|-------|----------------|\n",
    "| The   | [1, 0, 0, 0, 0, 0] |\n",
    "| dog   | [0, 0, 0, 0, 1, 0] |\n",
    "| is    | [0, 0, 1, 0, 0, 0] |\n",
    "| happy | [0, 0, 0, 0, 0, 1] |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the limitations of the above representations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words loses the order of the words while one-hot encoding is very demanding in size. Bag-of-words also loses the meaning of polysemous words like \"can\", that need to be understood in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Example of word embedding techniques, such as Word2Vec and GloVe are considered *dense* representations. How do dense word embeddings relate to the *distributional hypothesis*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distributional Hypothesis is a fundamental concept in linguistics and natural language processing (NLP) that suggests words that occur in similar contexts tend to have similar meanings.\n",
    "\n",
    "Embedding technipues captures these semantic relationships by placing words that are similiar to each other close on a high dimensional plane. This technique can capture that \"Iceland\" and \"vikings\" often appear together, also \"Iceland\" and \"countries\". But the words \"vikings\" and \"countries\" will be further apart since there will be fewer co-occurences of these words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
